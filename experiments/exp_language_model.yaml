model:
  emb_dim: 96
  d_x: 96
  d_h: 192
  layers: 2
  ssm_N: 96
  d_k: 96
  d_v: 96
  rank_kcm: 48
  kcm_consolidation_rate: 0.05
  kcm_reduce: "sum"
  use_rope: true
  auto_write: true
  use_diag_A: true
  res_dropout_p: 0.05
  memory_dtype: "float32"

train:
  batch_size: 32
  lr: 0.0001
  epochs: 10
  grad_clip: 1.0
  checkpoint_every: 3
  betas: [0.9, 0.98]
  dropout: 0.05
  scheduler: cosine

data:
  dataset_path: "enwik8_10M.txt"
  max_len: 128
  num_workers: 2
  data_mode: char