model:
  # --- Core Dimensions ---
  emb_dim: 32                # smaller embedding for char-level task
  d_x: 32                    # input dimension to first layer
  d_h: 64                    # hidden size (was 128)
  layers: 1                  # single CRSD layer for faster training
  ssm_N: 32                  # smaller SSM state (was 64)

  # --- KCM Parameters ---
  d_k: 64                    # key dimension (smaller)
  d_v: 32                    # value dimension
  rank_kcm: 32               # low-rank memory (was 64)
  kcm_consolidation_rate: 0.10
  kcm_reduce: "sum"

  # --- Flags and Regularization ---
  use_rope: true
  auto_write: true
  use_diag_A: true
  res_dropout_p: 0.05        # lower dropout for small model
  memory_dtype: "float32"    # safer and faster on MPS

train:
  batch_size: 16             # smaller batch for MPS memory limits
  lr: 0.0005                 # slightly higher for faster convergence
  epochs: 2
  grad_clip: 1.0
  checkpoint_every: 1
  betas: [0.9, 0.98]
  dropout: 0.05
  scheduler: cosine

data:
  dataset_path: "enwik8_1M.txt"  # smaller subset (~1M chars)
  max_len: 128                   # shorter sequences for faster steps
  num_workers: 2
  data_mode: char
